<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="google-site-verification" content="HERtZo9Bh-TSa7tDiURr5_MoTUZcMLPALIF0B-D-x24" />
    <title>Publications</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width">
    <base target="_blank">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">

      <section style="width: auto;">
        <div><a href="./index.html" target="_self">&lt; Back</a></div>

        <h2 class="section-title">Publications</h2>

        <div class="pub">
            <span class="title"><a href="https://arxiv.org/abs/2306.04590">Proximity-Informed Calibration for Deep Neural Networks</a></span>
            <span class="authors">Miao Xiong, <span class="author-tag">Ailin Deng</span>, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi </span>
            <span class="info"> NeurIPS(spotlight), 2023. <a class="codelink" href="https://github.com/MiaoXiong2320/ProximityBias-Calibration"></a> </span>
        </div>

        <div class="pub">
            <span class="title"><a href="#">Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection</a></span>
            <span class="authors">Jiaying Wu, Shen Li, <span class="author-tag">Ailin Deng</span>, Miao Xiong and Bryan Hooi</span>
            <span class="info"> CIKM, 2023. <a class="codelink" href="https://github.com/jiayingwu19/Prompt-and-Align"></a> </span>
        </div>

        <div class="pub">
          <span class="title"><a href="https://arxiv.org/abs/2305.01481">Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement</a></span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Miao Xiong, Bryan Hooi </span>
          <span class="info"> ICML, 2023. <a class="codelink" href="https://github.com/d-ailin/latent-agreement"></a> </span>
        </div>

        <div class="pub">
          <span class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf">Probabilistic Knowledge Distillation for Face Ensembles</a> </span>
          <span class="authors">Jianqing Xu, Shen Li, <span class="author-tag">Ailin Deng</span>, Miao Xiong, Jiaying Wu, Jiaxiang Wu, Shouhong Ding, Bryan Hooi </span>
          <span class="info"> CVPR, 2023.</span>
        </div>


        <div class="pub">
          <span class="title"> <a href="https://arxiv.org/abs/2302.02628">Trust, but Verify: Using Self-Supervised Probing to Improve Trustworthiness</a></span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Shen Li, Miao Xiong, Zhirui Chen, Bryan Hooi</span>
          <span class="info"> ECCV, 2022. <a class="codelink" href="https://github.com/d-ailin/ssprobing"></a> </span>
          <span class="toggle-content">
            Trustworthy machine learning is of primary importance to the practical deployment of deep learning models. While state-of-the-art models achieve astonishingly good performance in terms of accuracy, recent literature reveals that their predictive confidence scores unfortunately cannot be trusted: e.g., they are often overconfident when wrong predictions are made, or so even for obvious outliers. In this paper, we introduce a new approach of self-supervised probing, which enables us to check and mitigate the overconfidence issue for a trained model, thereby improving its trustworthiness. We provide a simple yet effective framework, which can be flexibly applied to existing trustworthiness-related methods in a plug-and-play manner. Extensive experiments on three trustworthiness-related tasks (misclassification detection, calibration and out-of-distribution detection) across various benchmarks verify the effectiveness of our proposed probing framework.          </span>
        </div>

        <div class="pub">
          <span class="title"><a href="https://openreview.net/forum?id=p5V8P2J61u">Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation</a>  </span>
          <span class="authors">Miao Xiong, Shen Li, Wenjie Feng, <span class="author-tag">Ailin Deng</span>, Jihai Zhang, Bryan Hooi</span>
          <span class="info"> TMLR, 2022. <a class="codelink" href="https://github.com/MiaoXiong2320/NeighborAgg"></a> </span>
          <span class="toggle-content">
            How do we know when the predictions made by a classifier can be trusted? This is a fundamental problem that also has immense practical applicability, especially in safety-critical areas such as medicine and autonomous driving. The de facto approach of using the classifier's softmax outputs as a proxy for trustworthiness suffers from the over-confidence issue; while the most recent works incur problems such as additional retraining cost and accuracy versus trustworthiness trade-off. In this work, we argue that the trustworthiness of a classifier's prediction for a sample is highly associated with two factors: the sample's neighborhood information and the classifier's output. To combine the best of both worlds, we design a model-agnostic post-hoc approach NeighborAGG to leverage the two essential information via an adaptive neighborhood aggregation. Theoretically, we show that NeighborAGG is a generalized version of a one-hop graph convolutional network, inheriting the powerful modeling ability to capture the varying similarity between samples within each class. We also extend our approach to the closely related task of mislabel detection and provide a theoretical coverage guarantee to bound the false negative. Empirically, extensive experiments on image and tabular benchmarks verify our theory and suggest that NeighborAGG outperforms other methods, achieving state-of-the-art trustworthiness performance.
          </span>

        </div>

        <div class="pub">
          <span class="title"><a href="https://www.ijcai.org/proceedings/2022/0278.pdf">CADET: Calibrated Anomaly Detection for Mitigating Hardness Bias</a> </span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Adam Goodge, Lang Yi Ang, Bryan Hooi</span>
          <span class="info"> IJCAI, 2022. <a class="codelink" href="https://github.com/d-ailin/CADET"></a> </span>
          <span class="toggle-content">
            The detection of anomalous samples in large,
            high-dimensional datasets is a challenging task
            with numerous practical applications. Recently,
            state-of-the-art performance is achieved with
            deep learning methods: for example, using the
            reconstruction error from an autoencoder as
            anomaly scores. However, the scores are uncalibrated: that is, they follow an unknown distribution and lack a clear interpretation. Furthermore, the reconstruction error is highly influenced by the ‘hardness’ of a given sample, which
            leads to false negative and false positive errors.
            In this paper, we empirically show the significance of this hardness bias present in a range
            of recent deep anomaly detection methods. To
            mitigate this, we propose an efficient and plugand-play error calibration method which mitigates this hardness bias in the anomaly scoring
            without the need to retrain the model. We verify
            the effectiveness of our method on a range of image, time-series, and tabular datasets and against
            several baseline methods.
          </span>
        </div>

        <div class="pub">
          <span class="title"><a href="https://arxiv.org/abs/2106.06947">Graph Neural Network-Based Anomaly Detection in Multivariate Time Series</a></span>
          <span class="authors"> <span class="author-tag">Ailin Deng</span>, Bryan Hooi</span>
          <span class="info"> AAAI, 2021. <a class="codelink" href="https://github.com/d-ailin/GDN"></a> </span>
          <span class="toggle-content">
            Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events, such as system faults and attacks? More challengingly, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships? Recently, deep learning approaches have enabled improvements in anomaly detection in high-dimensional datasets; however, existing methods do not explicitly learn the structure of existing relationships between variables, or use them to predict the expected behavior of time series. Our approach combines a structure learning approach with graph neural networks, additionally using attention weights to provide explainability for the detected anomalies. Experiments on two real-world sensor datasets with ground truth anomalies show that our method detects anomalies more accurately than baseline approaches, accurately captures correlations between sensors, and allows users to deduce the root cause of a detected anomaly.
          </span>
        </div>

    </div>
    <script src="javascripts/scale.fix.js"></script>

    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-SC04EMLTM5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-SC04EMLTM5');
  </script>

  <script
  src="https://code.jquery.com/jquery-3.6.4.slim.min.js"
  integrity="sha256-a2yjHM4jnF9f54xUQakjZGaqYs/V1CYvWpoqZzC2/Bw="
  crossorigin="anonymous"></script>
  <script>
    $('document').ready(function(){
      $('.toggle-trigger').append(' <i class="fa fa-caret-right" aria-hidden="true"></i>')
    })

    $('.toggle-trigger').click(function(e) {
      e.preventDefault();
      // Get the extra options directly after the clicked link
      var extraOptions = $(this).closest('div.pub').find('.toggle-content');

      if(extraOptions.is(":visible")) {
          extraOptions.hide()
          $(this).find('i').toggleClass('fa-caret-right').toggleClass('fa-caret-down')

      } else {
          extraOptions.css('display', 'block')
          $(this).find('i').toggleClass('fa-caret-right').toggleClass('fa-caret-down')

      };
    })
  </script>
  </body>
</html>
