<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="google-site-verification" content="HERtZo9Bh-TSa7tDiURr5_MoTUZcMLPALIF0B-D-x24" />
    <title>Ailin Deng @ NUS</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="title">Ailin Deng</h1>

        <img class="avatar" src="./assets/me.jpg">

        <!-- <p class="centered"> <i class="fa fa-map-marker"></i> Singapore </p>   -->

        <div class="icon-bar">
          <a href="mailto:ailin@u.nus.edu"><i class="fa fa-envelope" aria-hidden="true"></i></a>
          <a href="https://scholar.google.com/citations?user=VmhnpkUAAAAJ"><i class="fa fa-graduation-cap" aria-hidden="true"></i></a>
          <a href="https://github.com/d-ailin"><i class="fa fa-github" aria-hidden="true" ></i></a>
        </div>
      </header>
      <section class="content">

        <p>Hi! I am a PhD student in Computer Science at National University of Singapore (NUS), advised by <a href="https://bhooi.github.io">Bryan Hooi</a>.

          <br><br>
          My research focuses on trustworthy machine learning and my research goal is to build systems in an efficient, interpretable and reliable way. 
          I am interested in understanding why failures happen, how to detect them and using these insights to build machine learning systems more intelligently.
   
        <p> I have a broad interest in: failure prediction, calibration, anomaly detection, data biases, representation learning, uncertainty, interpretability, generalization, etc.
        </p>

        <p> Feel free to reach out (<a href="mailto:ailin@u.nus.edu" style="color: #595959"><i class="fa fa-envelope" aria-hidden="true"></i></a>) if you are interested in my research or seeking for potential collaboration :) </p>

        <h2 class="section-title"><i class="fa fa-book" aria-hidden="true"></i> Publications</h2>

        <div class="pub">
          <span class="title">Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement</span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Miao Xiong, Bryan Hooi </span>
          <span class="info"> ICML 2023</span>
          <span>
            <a class="toggle-trigger" href="#">abstract</a> / 
            <a href="https://arxiv.org/abs/2305.01481">paper</a> /
            <a href="https://github.com/d-ailin/latent-agreement">code</a>
          </span>
          <span class="toggle-content">
            Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring the agreement between its latent space, and the latent space of a foundation model. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, \eg, arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a neighborhood agreement measure between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability. Theoretical analysis and extensive experiments on failure detection across various datasets verify the effectiveness of our method on both in-distribution and out-of-distribution settings.
          </span>
        </div>

        <div class="pub">
          <span class="title">Probabilistic Knowledge Distillation for Face Ensembles</span>
          <span class="authors">Jianqing Xu, Shen Li, <span class="author-tag">Ailin Deng</span>, Miao Xiong, Jiaying Wu, Jiaxiang Wu, Shouhong Ding, Bryan Hooi </span>
          <span class="info"> CVPR 2023</span>
          <span>
            <a class="toggle-trigger" href="#">abstract</a> / 
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf">paper</a>
          </span>
          <span class="toggle-content">
            Mean ensemble (i.e. averaging predictions from multiple
            models) is a commonly-used technique in machine learning
            that improves the performance of each individual model. We
            formalize it as feature alignment for ensemble in open-set
            face recognition and generalize it into Bayesian Ensemble
            Averaging (BEA) through the lens of probabilistic modeling.
            This generalization brings up two practical benefits that existing methods could not provide: (1) the uncertainty of a
            face image can be evaluated and further decomposed into
            aleatoric uncertainty and epistemic uncertainty, the latter
            of which can be used as a measure for out-of-distribution
            detection of faceness; (2) a BEA statistic provably reflects
            the aleatoric uncertainty of a face image, acting as a measure for face image quality to improve recognition performance. To inherit the uncertainty estimation capability from
            BEA without the loss of inference efficiency, we propose
            BEA-KD, a student model to distill knowledge from BEA.
            BEA-KD mimics the overall behavior of ensemble members
            and consistently outperforms SOTA knowledge distillation
            methods on various challenging benchmarks.
          </span>
        </div>


        <div class="pub">
          <span class="title">Trust, but Verify: Using Self-Supervised Probing to Improve Trustworthiness  </span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Shen Li, Miao Xiong, Zhirui Chen, Bryan Hooi</span>
          <span class="info"> ECCV 2022</span>
          <span>
              <a class="toggle-trigger" href="#">abstract</a> /
              <a href="https://arxiv.org/abs/2302.02628">paper</a> /
              <a href="https://github.com/d-ailin/ssprobing">code</a>
          </span>
          <span class="toggle-content">
            Trustworthy machine learning is of primary importance to the practical deployment of deep learning models. While state-of-the-art models achieve astonishingly good performance in terms of accuracy, recent literature reveals that their predictive confidence scores unfortunately cannot be trusted: e.g., they are often overconfident when wrong predictions are made, or so even for obvious outliers. In this paper, we introduce a new approach of self-supervised probing, which enables us to check and mitigate the overconfidence issue for a trained model, thereby improving its trustworthiness. We provide a simple yet effective framework, which can be flexibly applied to existing trustworthiness-related methods in a plug-and-play manner. Extensive experiments on three trustworthiness-related tasks (misclassification detection, calibration and out-of-distribution detection) across various benchmarks verify the effectiveness of our proposed probing framework.          </span>
        </div>

        <div class="pub">
          <span class="title"> Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation </span>
          <span class="authors">Miao Xiong, Shen Li, Wenjie Feng, <span class="author-tag">Ailin Deng</span>, Jihai Zhang, Bryan Hooi</span>
          <span class="info"> TMLR 2022</span>
          <span>
            <a class="toggle-trigger" href="#">abstract</a> /
            <a href="https://openreview.net/forum?id=p5V8P2J61u">paper</a> / 
            <a href="https://github.com/MiaoXiong2320/NeighborAgg">code</a>
          </span>
          <span class="toggle-content">
            How do we know when the predictions made by a classifier can be trusted? This is a fundamental problem that also has immense practical applicability, especially in safety-critical areas such as medicine and autonomous driving. The de facto approach of using the classifier's softmax outputs as a proxy for trustworthiness suffers from the over-confidence issue; while the most recent works incur problems such as additional retraining cost and accuracy versus trustworthiness trade-off. In this work, we argue that the trustworthiness of a classifier's prediction for a sample is highly associated with two factors: the sample's neighborhood information and the classifier's output. To combine the best of both worlds, we design a model-agnostic post-hoc approach NeighborAGG to leverage the two essential information via an adaptive neighborhood aggregation. Theoretically, we show that NeighborAGG is a generalized version of a one-hop graph convolutional network, inheriting the powerful modeling ability to capture the varying similarity between samples within each class. We also extend our approach to the closely related task of mislabel detection and provide a theoretical coverage guarantee to bound the false negative. Empirically, extensive experiments on image and tabular benchmarks verify our theory and suggest that NeighborAGG outperforms other methods, achieving state-of-the-art trustworthiness performance.
          </span>

        </div>

        <div class="pub">
          <span class="title"> CADET: Calibrated Anomaly Detection for Mitigating Hardness Bias </span>
          <span class="authors"><span class="author-tag">Ailin Deng</span>, Adam Goodge, Lang Yi Ang, Bryan Hooi</span>
          <span class="info"> IJCAI 2022</span>
          <span>
            <a class="toggle-trigger" href="#">abstract</a> /
            <a href="https://www.ijcai.org/proceedings/2022/0278.pdf">paper</a> /
            <a href="https://github.com/d-ailin/CADET">code</a>
          </span>
          <span class="toggle-content">
            The detection of anomalous samples in large,
            high-dimensional datasets is a challenging task
            with numerous practical applications. Recently,
            state-of-the-art performance is achieved with
            deep learning methods: for example, using the
            reconstruction error from an autoencoder as
            anomaly scores. However, the scores are uncalibrated: that is, they follow an unknown distribution and lack a clear interpretation. Furthermore, the reconstruction error is highly influenced by the ‘hardness’ of a given sample, which
            leads to false negative and false positive errors.
            In this paper, we empirically show the significance of this hardness bias present in a range
            of recent deep anomaly detection methods. To
            mitigate this, we propose an efficient and plugand-play error calibration method which mitigates this hardness bias in the anomaly scoring
            without the need to retrain the model. We verify
            the effectiveness of our method on a range of image, time-series, and tabular datasets and against
            several baseline methods.
          </span>
        </div>

        <div class="pub">
          <span class="title"> Graph Neural Network-Based Anomaly Detection in Multivariate Time Series </span>
          <span class="authors"> <span class="author-tag">Ailin Deng</span>, Bryan Hooi</span>
          <span class="info"> AAAI 2021</span>
          <span>
            <a class="toggle-trigger" href="#">abstract</a> /
            <a href="https://arxiv.org/abs/2106.06947">paper</a> / 
            <a href="https://github.com/d-ailin/GDN">code</a>
          </span>
          <span class="toggle-content">
            Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events, such as system faults and attacks? More challengingly, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships? Recently, deep learning approaches have enabled improvements in anomaly detection in high-dimensional datasets; however, existing methods do not explicitly learn the structure of existing relationships between variables, or use them to predict the expected behavior of time series. Our approach combines a structure learning approach with graph neural networks, additionally using attention weights to provide explainability for the detected anomalies. Experiments on two real-world sensor datasets with ground truth anomalies show that our method detects anomalies more accurately than baseline approaches, accurately captures correlations between sensors, and allows users to deduce the root cause of a detected anomaly.
          </span>
        </div>

        <h2 class="section-title"><i class="fa fa-cogs" aria-hidden="true"></i> Service</h2>
        <b>Reviewer</b>
        <br>
        (Journal) TKDE, Artificial Intelligence Review, TETCI, TNNLS, ACM Computing Surveys
        <br>
        (Conference) ICML'22, Neurips'23
        <h2 class="section-title"><i class="fa fa-star" aria-hidden="true"></i> Selected Awards</h2>
        ICML Volunteer Grant 2023<br>
        NUS SOC Research Achievement Award x 2 (Sem 2 2020/2021, Sem 2 2021/2022)


        <h2 class="section-title"><i class="fa fa-users" aria-hidden="true"></i> Teaching</h2>
        Teacher Assistant, Big-Data Analytics Technology (CS5344), NUS, 2021 and 2022.


      </section>
      <footer>
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>

    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-SC04EMLTM5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-SC04EMLTM5');
  </script>

  <script
  src="https://code.jquery.com/jquery-3.6.4.slim.min.js"
  integrity="sha256-a2yjHM4jnF9f54xUQakjZGaqYs/V1CYvWpoqZzC2/Bw="
  crossorigin="anonymous"></script>
  <script>
    $('document').ready(function(){
      $('.toggle-trigger').append(' <i class="fa fa-caret-right" aria-hidden="true"></i>')
    })

    $('.toggle-trigger').click(function(e) {
      e.preventDefault();
      // Get the extra options directly after the clicked link
      var extraOptions = $(this).closest('div.pub').find('.toggle-content');

      if(extraOptions.is(":visible")) {
          extraOptions.hide()
          $(this).find('i').toggleClass('fa-caret-right').toggleClass('fa-caret-down')

      } else {
          extraOptions.css('display', 'block')
          $(this).find('i').toggleClass('fa-caret-right').toggleClass('fa-caret-down')

      };
    })
  </script>
  </body>
</html>
